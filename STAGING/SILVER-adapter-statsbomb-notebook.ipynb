{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "# NOTEBOOK CONFIG\n\n%idle_timeout 1440\n%glue_version 4.0\n%worker_type Standard\n%number_of_workers 2",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 41db922a-2d7f-4be5-b6cd-7a0e5cd69339.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Current idle_timeout is None minutes.\nidle_timeout has been set to 1440 minutes.\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 41db922a-2d7f-4be5-b6cd-7a0e5cd69339.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Setting Glue version to: 4.0\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 41db922a-2d7f-4be5-b6cd-7a0e5cd69339.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous worker type: None\nSetting new worker type to: Standard\n",
					"output_type": "stream"
				},
				{
					"name": "stderr",
					"text": "You are already connected to a glueetl session 41db922a-2d7f-4be5-b6cd-7a0e5cd69339.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
					"output_type": "stream"
				},
				{
					"name": "stdout",
					"text": "Previous number of workers: None\nSetting new number of workers to: 2\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "###\nseasons_to_update = [\"046281\"]\nclient_id =  8\n# db_cluster_arn = \"arn:aws:rds:eu-west-1:127023367472:cluster:traitsproddb\"\n# db_credentials_secret_store_arn = \"arn:aws:secretsmanager:eu-west-1:127023367472:secret:traits-prod/aurora/mysql-aHJc2o\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 96,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Libraries and config\nimport ast\nimport re\nimport sys\nimport json\nimport boto3\nimport pandas as pd\nimport pyspark.sql.functions as F\nfrom datetime import datetime\nfrom time import time\nfrom pyspark.sql import SparkSession\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.job import Job\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import explode, concat, col, lit, udf, expr, array_max, collect_list, sum as _sum, year, to_date, first, struct, input_file_name, regexp_extract, when, concat, countDistinct, rank\nfrom pyspark.sql.types import IntegerType, StringType, ArrayType, StructType\nfrom datetime import datetime, timedelta\nfrom s3fs import S3FileSystem\nimport warnings\n\n### Initialise clients\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\ns3 = S3FileSystem()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 97,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ### Variables ###\n\n# # Retrieve arguments passed from STEP FUNCTION map as parameter\n# # NB: Wyscout Data Warehouse is centralised, so client_id can be used as a proxy for production / other replication\n# try:\n#     args = getResolvedOptions(sys.argv, ['client_id', 'seasons_to_update'])\n#     client_id = args['client_id']\n# except:\n#     args = getResolvedOptions(sys.argv, ['default_client_id', 'seasons_to_update'])\n#     client_id = args['default_client_id']\n    \n# # Convert to strings\n# seasons_to_update = [str(s) for s in ast.literal_eval(args['seasons_to_update'])]\n    \n# Threshold for valid games contributing to a profile sample\nplaytime_threshold = 33",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 98,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Columns to pass to output in addition to features\nmeta_cols = ['profileId',\n'playerId',\n'teamId',\n'competitionId',\n'seasonId',\n'seasonPartition',\n'startYear',\n'endYear',\n'age',\n'playerName',\n'teamName',\n'seasonName',\n'seasonDisplayName',\n'competitionName',\n'competitionShortName',\n'fullName',\n'nationality',\n'birthDate',\n'positionGroup',\n'positionName',\n'positionAbbreviation',\n'teamSeason',\n'playerTeamSeason',\n'playerTeamSeasonCompetition',\n]\n\ninteger_columns = ['startYear',\n'endYear',\n'age',\n'totalMinutesInSample',\n'sampleSize',\n'totalMinutesForSeason',\n'appearancesForSeason']\n\nstring_columns = ['profileId',\n'playerId',\n'teamId',\n'competitionId',\n'seasonId',\n'seasonPartition',\n'playerName',\n'teamName',\n'seasonName',\n'seasonDisplayName',\n'competitionName',\n'competitionShortName',\n'fullName',\n'nationality',\n'birthDate',\n'positionGroup',\n'positionName',\n'positionAbbreviation',\n'teamSeason',\n'playerTeamSeason',\n'playerTeamSeasonCompetition',\n'aggregationPeriod']",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 99,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Helper functions ###\n\ndef flatten_struct(schema, prefix=None):\n    \"\"\"Generate a list of column expressions that flatten a DataFrame schema with prefixed nested column names.\"\"\"\n    field_exprs = []\n    for field in schema.fields:\n        name = field.name\n        dtype = field.dataType\n        current_col = f\"`{name}`\" if prefix is None else f\"`{prefix}`.`{name}`\"\n        current_prefix = name if prefix is None else f\"{prefix}_{name}\"\n        \n        if isinstance(dtype, StructType):\n            # If the field is a StructType, recurse to flatten it\n            nested_exprs = flatten_struct(dtype, prefix=current_prefix)\n            field_exprs.extend(nested_exprs)\n        else:\n            # For non-struct fields, alias the column with its prefixed name\n            field_exprs.append(col(current_col).alias(current_prefix))\n    return field_exprs\n\ndef calculate_age(birthDate, year):\n    today = datetime.today()\n    try:\n        birthDate_obj = datetime.strptime(birthDate, '%Y-%m-%d')\n        if today.year == year:\n            age = today.year - birthDate_obj.year\n            if today.month < birthDate_obj.month or (today.month == birthDate_obj.month and today.day < birthDate_obj.day):\n                age -= 1\n        else:\n            age = int(year) - birthDate_obj.year\n        return age\n    except:\n        return None\n        \n# def get_national_team(team_id, name):\n#     return national_teams.get(str(team_id), name)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 100,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Pre-load merge tables [retrieved information] ###\n\n# Position mapping dictionary\ntry:\n    with s3.open(f's3://traits-app/settings/positions_map.json', 'r', encoding='utf-8') as f:\n        pos_key = json.load(f)['STATSBOMB']\n    pos_key = {k: oldk for oldk, oldv in pos_key.items() for k in oldv}\nexcept FileNotFoundError:\n    print(\"positions_map.json file not found\")\nexcept ValueError:\n    print(\"positions_map.json file is not in the expected format\")\n    \n# Competitions [id, competition name]\ncomps_df = pd.read_json('s3://traits-app/settings/statsbomb_competitions.json', encoding='utf-8-sig')\n\n# # National teams lookups\n# with s3.open('s3://traits-app/bronze-raw/wyscout/national_teams.json', 'r', encoding='utf-8') as file:\n#     national_teams = json.load(file)\n\n# # Players\n# player_path = f's3://traits-app/deployments/{client_id}/players.csv'\n# player_df = pd.read_csv(player_path).drop_duplicates(subset=['offline_player_id'], keep='last')\n# player_df['offline_player_id'] = player_df['offline_player_id'].astype('str')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 101,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### ETL PROCESSING ###    \n\nprint(f\"Parsing seasons: {seasons_to_update}\")\n\nstart = time()\n\n# Read player statistics from json\nplayer_s3_path = \"s3://traits-app/bronze-raw/statsbomb/8/{\" + \",\".join(seasons_to_update) + \"}/matches/*.json\"\ndf_matches = spark.read.json(player_s3_path)\n\n# Printing the \"shape\" of the DataFrame\ndef print_shape(df):\n    print(f\"Matches shape: Rows = {df_matches.count()}, Columns = {len(df_matches.columns)}\")\n    print(\"Games in selected seasons:\", df_matches.select(\"match_id\").distinct().count())\n    \nprint_shape(df_matches)\n\n# # Specify meta columns to include\n# include_cols = ['seasonId', 'competitionId', 'roundId', 'matchId', 'playerId', 'positions',\n#                 'player.birthDate', 'player.currentNationalTeamId', 'player.currentTeamId', 'player.shortName',\n#                 'player.firstName', 'player.middleName', 'player.lastName', 'player.foot',\n#                 'player.height', 'player.weight', 'player.gender',\n#                 'player.imageDataURL', 'player.birthArea.name', 'player.role', 'player.status']\n\n# # Generate the column expressions for the DataFrame\n# flatten_exprs = [col(name) for name in include_cols] +\\\n#                 flatten_struct(df_matches.schema[\"total\"].dataType, prefix=\"total\") +\\\n#                 flatten_struct(df_matches.schema[\"average\"].dataType, prefix=\"average\") +\\\n#                 flatten_struct(df_matches.schema[\"percent\"].dataType, prefix=\"percent\")\n\n# # Apply the expressions to select and alias the columns\n# df_matches = df_matches.select(*flatten_exprs)\n\n# Drop NA playerIds\n# df_matches = df_matches.where(df_matches.player_id.isNotNull())\n\n### Aggregate selected events into match level statistics\n# NB: initially this is restricted to the statistics that appear in Statsbomb's season-level API\n\n# Read in event derivations\nevent_derivations = pd.read_csv(f's3://traits-app/settings/stats_catalogue.csv').dropna(subset=['sql'])\n\n# Start building the SQL query\nsql_query_events = \"SELECT match_id, player_id\"\n\n# Iterate through the DataFrame to populate the dictionary\nfor _, row in event_derivations.iterrows():\n    stat_name = row['name_source']\n    if pd.notna(row['sql']):  # Ensure there is a base SQL to work with\n        sql_query_events += f\", {row['sql']} AS {stat_name}\"\n    else:\n        # raise\n        print(ValueError(f\"Valid base_sql does not exist for statistic '{stat_name}'\"))\n\n# Finish the SQL query by specifying the source and grouping condition\nsql_query_events += \" FROM {0} GROUP BY match_id, player_id\"\n\n# read events into spark, flatten, and derive x and y locations into 'location_x' and 'location_y'\nevents_s3_path = \"s3://traits-app/bronze-raw/statsbomb/8/{\" + \",\".join(seasons_to_update) + \"}/events/*.json\"\ndf_events = spark.read.json(events_s3_path).select(\n    #col(\"50_50.outcome.id\").alias(\"50_50_outcome_id\"),\n    #col(\"50_50.outcome.name\").alias(\"50_50_outcome_name\"),\n    #col(\"bad_behaviour.card.id\").alias(\"bad_behaviour_card_id\"),\n    #col(\"bad_behaviour.card.name\").alias(\"bad_behaviour_card_name\"),\n    col(\"ball_receipt.outcome.id\").alias(\"ball_receipt_outcome_id\"),\n    col(\"ball_receipt.outcome.name\").alias(\"ball_receipt_outcome\"),\n    col(\"ball_recovery.offensive\").alias(\"ball_recovery_offensive\"),\n    col(\"ball_recovery.recovery_failure\").alias(\"ball_recovery_recovery_failure\"),\n    col(\"block.deflection\").alias(\"block_deflection\"),\n    col(\"block.offensive\").alias(\"block_offensive\"),\n    col(\"block.save_block\").alias(\"block_save_block\"),\n    col(\"carry.end_location\").alias(\"carry_end_location\"),\n    #col(\"clearance.aerial_won\").alias(\"clearance_aerial_won\"),\n    #col(\"clearance.body_part.id\").alias(\"clearance_body_part_id\"),\n    #col(\"clearance.body_part.name\").alias(\"clearance_body_part_name\"),\n    #col(\"clearance.head\").alias(\"clearance_head\"),\n    #col(\"clearance.left_foot\").alias(\"clearance_left_foot\"),\n    #col(\"clearance.other\").alias(\"clearance_other\"),\n    #col(\"clearance.right_foot\").alias(\"clearance_right_foot\"),\n    col(\"counterpress\").alias(\"counterpress\"),\n    col(\"dribble.no_touch\").alias(\"dribble_no_touch\"),\n    col(\"dribble.nutmeg\").alias(\"dribble_nutmeg\"),\n    col(\"dribble.outcome.id\").alias(\"dribble_outcome_id\"),\n    col(\"dribble.outcome.name\").alias(\"dribble_outcome\"),\n    col(\"dribble.overrun\").alias(\"dribble_overrun\"),\n    col(\"duel.outcome.id\").alias(\"duel_outcome_id\"),\n    col(\"duel.outcome.name\").alias(\"duel_outcome\"),\n    col(\"duel.type.id\").alias(\"duel_type_id\"),\n    col(\"duel.type.name\").alias(\"duel_type\"),\n    col(\"duration\").alias(\"duration\"),\n    col(\"foul_committed.advantage\").alias(\"foul_committed_advantage\"),\n    col(\"foul_committed.card.id\").alias(\"foul_committed_card_id\"),\n    col(\"foul_committed.card.name\").alias(\"foul_committed_card_name\"),\n    col(\"foul_committed.offensive\").alias(\"foul_committed_offensive\"),\n    col(\"foul_committed.penalty\").alias(\"foul_committed_penalty\"),\n    col(\"foul_committed.type.id\").alias(\"foul_committed_type_id\"),\n    col(\"foul_committed.type.name\").alias(\"foul_committed_type_name\"),\n    col(\"foul_won.advantage\").alias(\"foul_won_advantage\"),\n    col(\"foul_won.defensive\").alias(\"foul_won_defensive\"),\n    col(\"foul_won.penalty\").alias(\"foul_won_penalty\"),\n    #col(\"goalkeeper.body_part.id\").alias(\"goalkeeper_body_part_id\"),\n    #col(\"goalkeeper.body_part.name\").alias(\"goalkeeper_body_part_name\"),\n    #col(\"goalkeeper.end_location\").alias(\"goalkeeper_end_location\"),\n    #col(\"goalkeeper.lost_in_play\").alias(\"goalkeeper_lost_in_play\"),\n    #col(\"goalkeeper.outcome.id\").alias(\"goalkeeper_outcome_id\"),\n    #col(\"goalkeeper.outcome.name\").alias(\"goalkeeper_outcome_name\"),\n    #col(\"goalkeeper.position.id\").alias(\"goalkeeper_position_id\"),\n    #col(\"goalkeeper.position.name\").alias(\"goalkeeper_position_name\"),\n    #col(\"goalkeeper.punched_out\").alias(\"goalkeeper_punched_out\"),\n    #col(\"goalkeeper.shot_saved_off_target\").alias(\"goalkeeper_shot_saved_off_target\"),\n    #col(\"goalkeeper.shot_saved_to_post\").alias(\"goalkeeper_shot_saved_to_post\"),\n    #col(\"goalkeeper.technique.id\").alias(\"goalkeeper_technique_id\"),\n    #col(\"goalkeeper.technique.name\").alias(\"goalkeeper_technique_name\"),\n    #col(\"goalkeeper.type.id\").alias(\"goalkeeper_type_id\"),\n    #col(\"goalkeeper.type.name\").alias(\"goalkeeper_type_name\"),\n    #col(\"half_start.late_video_start\").alias(\"half_start_late_video_start\"),\n    col(\"id\").alias(\"id\"),\n    col(\"index\").alias(\"index\"),\n    col(\"injury_stoppage.in_chain\").alias(\"injury_stoppage_in_chain\"),\n    col(\"interception.outcome.id\").alias(\"interception_outcome_id\"),\n    col(\"interception.outcome.name\").alias(\"interception_outcome\"),\n    col(\"location\").alias(\"location\"),\n    col(\"minute\").alias(\"minute\"),\n    col(\"miscontrol.aerial_won\").alias(\"miscontrol_aerial_won\"),\n    #col(\"obv_against_after\").alias(\"obv_against_after\"),\n    #col(\"obv_against_before\").alias(\"obv_against_before\"),\n    #col(\"obv_against_net\").alias(\"obv_against_net\"),\n    #col(\"obv_for_after\").alias(\"obv_for_after\"),\n    #col(\"obv_for_before\").alias(\"obv_for_before\"),\n    #col(\"obv_for_net\").alias(\"obv_for_net\"),\n    #col(\"obv_total_net\").alias(\"obv_total_net\"),\n    #col(\"off_camera\").alias(\"off_camera\"),\n    col(\"out\").alias(\"out\"),\n    col(\"pass.aerial_won\").alias(\"pass_aerial_won\"),\n    col(\"pass.angle\").alias(\"pass_angle\"),\n    col(\"pass.assisted_shot_id\").alias(\"pass_assisted_shot_id\"),\n    col(\"pass.body_part.id\").alias(\"pass_body_part_id\"),\n    col(\"pass.body_part.name\").alias(\"pass_body_part_name\"),\n    col(\"pass.cross\").alias(\"pass_cross\"),\n    col(\"pass.cut_back\").alias(\"pass_cut_back\"),\n    col(\"pass.deflected\").alias(\"pass_deflected\"),\n    col(\"pass.end_location\").alias(\"pass_end_location\"),\n    col(\"pass.goal_assist\").alias(\"pass_goal_assist\"),\n    col(\"pass.height.id\").alias(\"pass_height_id\"),\n    col(\"pass.height.name\").alias(\"pass_height\"),\n    col(\"pass.inswinging\").alias(\"pass_inswinging\"),\n    col(\"pass.length\").alias(\"pass_length\"),\n    col(\"pass.miscommunication\").alias(\"pass_miscommunication\"),\n    col(\"pass.no_touch\").alias(\"pass_no_touch\"),\n    col(\"pass.outcome.id\").alias(\"pass_outcome_id\"),\n    col(\"pass.outcome.name\").alias(\"pass_outcome\"),\n    col(\"pass.outswinging\").alias(\"pass_outswinging\"),\n    #col(\"pass.pass_cluster_id\").alias(\"pass_pass_cluster_id\"),\n    #col(\"pass.pass_cluster_label\").alias(\"pass_pass_cluster_label\"),\n    #col(\"pass.pass_cluster_probability\").alias(\"pass_pass_cluster_probability\"),\n    #col(\"pass.pass_success_probability\").alias(\"pass_pass_success_probability\"),\n    col(\"pass.recipient.id\").alias(\"pass_recipient_id\"),\n    col(\"pass.recipient.name\").alias(\"pass_recipient_name\"),\n    col(\"pass.shot_assist\").alias(\"pass_shot_assist\"),\n    col(\"pass.straight\").alias(\"pass_straight\"),\n    col(\"pass.switch\").alias(\"pass_switch\"),\n    col(\"pass.technique.id\").alias(\"pass_technique_id\"),\n    col(\"pass.technique.name\").alias(\"pass_technique_name\"),\n    col(\"pass.through_ball\").alias(\"pass_through_ball\"),\n    col(\"pass.type.id\").alias(\"pass_type_id\"),\n    col(\"pass.type.name\").alias(\"pass_type\"),\n    col(\"period\").alias(\"period\"),\n    col(\"play_pattern.id\").alias(\"play_pattern_id\"),\n    col(\"play_pattern.name\").alias(\"play_pattern\"),\n    col(\"player.id\").alias(\"player_id\"),\n    col(\"player.name\").alias(\"player_name\"),\n    col(\"position.id\").alias(\"position_id\"),\n    col(\"position.name\").alias(\"position_name\"),\n    col(\"possession\").alias(\"possession\"),\n    col(\"possession_team.id\").alias(\"possession_team_id\"),\n    col(\"possession_team.name\").alias(\"possession_team_name\"),\n    col(\"related_events\").alias(\"related_events\"),\n    col(\"second\").alias(\"second\"),\n    #col(\"shot.aerial_won\").alias(\"shot_aerial_won\"),\n    #col(\"shot.body_part.id\").alias(\"shot_body_part_id\"),\n    #col(\"shot.body_part.name\").alias(\"shot_body_part_name\"),\n    #col(\"shot.deflected\").alias(\"shot_deflected\"),\n    #col(\"shot.end_location\").alias(\"shot_end_location\"),\n    #col(\"shot.first_time\").alias(\"shot_first_time\"),\n    #col(\"shot.follows_dribble\").alias(\"shot_follows_dribble\"),\n    #col(\"shot.gk_positioning_xg_suppression\").alias(\"shot_gk_positioning_xg_suppression\"),\n    #col(\"shot.gk_save_difficulty_xg\").alias(\"shot_gk_save_difficulty_xg\"),\n    #col(\"shot.gk_shot_stopping_xg_suppression\").alias(\"shot_gk_shot_stopping_xg_suppression\"),\n    #col(\"shot.key_pass_id\").alias(\"shot_key_pass_id\"),\n    # col(\"shot.one_on_one\").alias(\"shot_one_on_one\"),\n    # col(\"shot.open_goal\").alias(\"shot_open_goal\"),\n    # col(\"shot.outcome.id\").alias(\"shot_outcome_id\"),\n    # col(\"shot.outcome.name\").alias(\"shot_outcome\"),\n    # col(\"shot.redirect\").alias(\"shot_redirect\"),\n    # col(\"shot.saved_off_target\").alias(\"shot_saved_off_target\"),\n    # col(\"shot.saved_to_post\").alias(\"shot_saved_to_post\"),\n    # col(\"shot.shot_execution_xg\").alias(\"shot_shot_execution_xg\"),\n    # col(\"shot.shot_execution_xg_uplift\").alias(\"shot_shot_execution_xg_uplift\"),\n    # col(\"shot.shot_shot_assist\").alias(\"shot_shot_assist\"),\n    # col(\"shot.statsbomb_xg\").alias(\"shot_statsbomb_xg\"),\n    # col(\"shot.technique.id\").alias(\"shot_technique_id\"),\n    # col(\"shot.technique.name\").alias(\"shot_technique_name\"),\n    # col(\"shot.type.id\").alias(\"shot_type_id\"),\n    # col(\"shot.type.name\").alias(\"shot_type_name\"),\n    # col(\"substitution.outcome.id\").alias(\"substitution_outcome_id\"),\n    # col(\"substitution.outcome.name\").alias(\"substitution_outcome\"),\n    # col(\"substitution.replacement.id\").alias(\"substitution_replacement_id\"),\n    # col(\"substitution.replacement.name\").alias(\"substitution_replacement_name\"),\n    # col(\"tactics.formation\").alias(\"tactics_formation\"),\n    # col(\"tactics.lineup.jersey_number\").alias(\"tactics_lineup_jersey_number\"),\n    # col(\"tactics.lineup.player.id\").alias(\"tactics_lineup_player_id\"),\n    # col(\"tactics.lineup.player.name\").alias(\"tactics_lineup_player_name\"),\n    # col(\"tactics.lineup.position.id\").alias(\"tactics_lineup_position_id\"),\n    # col(\"tactics.lineup.position.name\").alias(\"tactics_lineup_position_name\"),\n    col(\"team.id\").alias(\"team_id\"),\n    col(\"team.name\").alias(\"team_name\"),\n    col(\"timestamp\").alias(\"timestamp\"),\n    col(\"type.id\").alias(\"type_id\"),\n    col(\"type.name\").alias(\"type\"),\n    col(\"under_pressure\").alias(\"under_pressure\")\n).withColumn(\"location_x\", col(\"location\")[0]) \\\n             .withColumn(\"location_y\", col(\"location\")[1]) \\\n             .drop(\"location\")\n\n# Add match_id column\ndf_events = df_events.withColumn(\"filename\", input_file_name())\n\n# Parse match id\ndf_events = df_events.withColumn(\"match_id\", regexp_extract(\"filename\", r\".*/(\\d+).json\", 1))\n\n# Merge team possessions for adj stats derivations\ndf_events = df_events.join(df_matches.select(['player_id', 'match_id', 'player_match_possession']), ['player_id', 'match_id'])\n\n# Create temp view\ndf_events.createOrReplaceTempView(\"events_matches\")\n\n# Apply the SQL statement\ndf_stats = spark.sql(sql_query_events.format(\"events_matches\"))\n\n# # TEMP: write stats data\n# df_stats.toPandas().to_csv(f's3://traits-app/silver-cleaned/{client_id}/derived_stats_{competition_id}_{season_id}.csv')\n\n# Merge derived stats onto matches dataframe\ndf_matches = df_matches.join(df_stats, ['match_id', 'player_id'])\n\nprint_shape(df_matches)\n\n# Enforce dtypes on input frame\n# for col_name in ['account_id', 'match_id', 'team_id', 'player_id', 'player_match_minutes', 'competition_id', 'season_id']:\n#     df_matches = df_matches.withColumn(col_name, col(col_name).cast(IntegerType()))\n# for col_name in ['team_name', 'player_name', 'competition_name', 'season_name']:\n#     df_matches = df_matches.withColumn(col_name, col(col_name).cast(StringType()))\n# # TODO: else float or double type",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 102,
			"outputs": [
				{
					"name": "stdout",
					"text": "Parsing seasons: ['046281']\nMatches shape: Rows = 9497, Columns = 156\nGames in selected seasons: 312\nMatches shape: Rows = 8263, Columns = 171\nGames in selected seasons: 272\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### PARSE LINEUPS FILES \n# 1. Cards information\n\n# Read the JSON files for lineups\nlineups_s3_path = \"s3://traits-app/bronze-raw/statsbomb/8/{\" + \",\".join(seasons_to_update) + \"}/lineups/*.json\"\ndf_lineups = spark.read.json(lineups_s3_path)\n\n# Add filename as column for match_id\ndf_lineups = df_lineups.withColumn(\"filename\", input_file_name())\n\n# Parse match id\ndf_lineups = df_lineups.withColumn(\"match_id\", regexp_extract(\"filename\", r\".*/(\\d+).json\", 1))\n\n# Further explode to get individual player data\ndf_exploded = df_lineups.selectExpr(\n    \"match_id\",\n    \"team_name\",\n    \"team_id\",\n    \"explode(lineup) as player_data\"\n)\n\nprint(\"Games in selected_seasons (lineups endpoint):\", df_lineups.select(\"match_id\").distinct().count())\n\n# Select the required fields\ndf_exploded = df_exploded.select(\n    \"match_id\",\n    \"player_data.player_id\", # merge col\n    \"player_data.player_name\",\n    \"player_data.positions\",\n    \"player_data.country\",\n    \"player_data.player_nickname\",\n    \"player_data.birth_date\",\n    \"player_data.player_gender\",\n    \"player_data.player_height\",\n    \"player_data.player_weight\",\n    \"player_data.jersey_number\"\n)\n\n# Create default player column\ndf_exploded = df_exploded.withColumn(\"default_name\", when(col(\"player_nickname\").isNotNull(), col(\"player_nickname\")).otherwise(col(\"player_name\")))\n\n# Parse the lineups df for yellow and red card events\ndf_lineup_events = df_lineups.select(\n    \"match_id\",\n    explode(\"events\").alias(\"event\")\n)\n\n# Filter events for \"Yellow Card\" and \"Red Card\"\ndf_filtered_events = df_lineup_events.filter(\n    (col(\"event.outcome\") == \"Yellow Card\") | (col(\"event.outcome\") == \"Red Card\")\n).select(\n    \"event.player_id\",\n    \"match_id\",\n    \"event.outcome\"\n)\n\n# Count occurrences of each type of card per player\ndf_yellow_cards = df_filtered_events.filter(col(\"outcome\") == \"Yellow Card\").groupBy(\"player_id\", \"match_id\").count().withColumnRenamed(\"count\", \"yellow_cards\")\ndf_red_cards = df_filtered_events.filter(col(\"outcome\") == \"Red Card\").groupBy(\"player_id\", \"match_id\").count().withColumnRenamed(\"count\", \"red_cards\")\n\n# Join the counts back to get a single row per player_id with counts for each card type\ndf_cards = df_yellow_cards.join(df_red_cards, [\"player_id\", \"match_id\"], \"outer\").fillna(0)\n\n# Merge all the match by match lineup info\ndf_info = df_exploded.drop(\"player_name\").join(df_cards.select([\"player_id\", \"match_id\", \"red_cards\", \"yellow_cards\"]), [\"player_id\", \"match_id\"], \"left\")\n\n# Fill na in merge table\nfill_values = {'player_height': 0, 'player_weight': 0, 'red_cards': 0, 'yellow_cards': 0}\ndf_info = df_info.fillna(fill_values)\n\n# 2. Positions information\n\n# Map the positionGroup to the Wyscout abbreviation the position map\nbroadcast_pos_map = spark.sparkContext.broadcast(pos_key)\n\ndef map_position_to_abbr(position_name):\n    return broadcast_pos_map.value.get(position_name, None)\n\nmap_position_to_abbr_udf = udf(map_position_to_abbr, StringType())\n\n# Extract the *first listed positionName and positionCode* from Wyscout's positions dictionary\n# TODO: OPTIONAL: implement more advanced logic that extracts the highest minutes position, rather than the starting position\ndf_info = df_info.withColumn(\"primary_position\", col(\"positions\")[0])\ndf_info = df_info.withColumn(\"nationality\", col(\"country.name\"))\ndf_info = df_info.withColumn(\"positionName\", col(\"primary_position.position\"))\ndf_info = df_info.withColumn(\"positionAbbreviation\", col(\"primary_position.position_id\"))\ndf_info = df_info.withColumn(\"positionGroup\", map_position_to_abbr_udf(col(\"positionName\")))\n\n# Show the original count in from the matches endpoint\nprint(\"Parse player match stats from lineups source:\")\nprint_shape(df_info)\n# SHOULD BE EQUAL TO df_matches shape if lineup data available\n\n# Merge card and player info to match stats frame on player_id and match_id\ndf_matches_merged = df_matches.join(df_info, [\"player_id\", \"match_id\"], \"left\")\n\n# TEMP: write match data\n# df_matches_merged.toPandas().to_csv(f's3://traits-app/silver-cleaned/{client_id}/matches_data_{competition_id}_{season_id}.csv')",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 103,
			"outputs": [
				{
					"name": "stdout",
					"text": "Games in selected_seasons (lineups endpoint): 273\nParse player match stats from lineups source:\nMatches shape: Rows = 8263, Columns = 171\nGames in selected seasons: 272\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Create \"Any\" designation\n\n# Create rows that will not apply gametime thresholds, marked as \"ANY\" in positionGroup, positionName, positionCode\nduplicated_df = df_matches_merged.withColumn(\"positionGroup\", F.lit(\"ANY\"))\\\n                .withColumn(\"positionName\", F.lit(\"ANY\"))\\\n                .withColumn(\"positionAbbreviation\", F.lit(\"ANY\"))\n\n# Append to original positions frame\ndf_matches_merged = df_matches_merged.unionByName(duplicated_df)\n\n# Drop rows where the total matchtime was under the threshold, but keep all \"ANY\" rows\n# NB: this IGNORES that occasionally total game minutes will be over the threshold, but time spent in primary position will not be\ndf_split = df_matches_merged.filter(\n    (col('player_match_minutes') > playtime_threshold) | (col('positionGroup') == \"ANY\")\n)\nprint(f\"Shape after dropping ineligible matches for all players frame: {df_split.count()}\")\n\n# ### OPTIONAL check for unique player per match rows\n\n# # Check to ensure there is only two rows per player per match\n# check_df = df_split.groupBy(\"playerId\", \"matchId\").count()\n\n# # # Filter to find invalid rows\n# invalid_rows_df = check_df.filter(check_df['count'] > 2)\n\n# # # Show the shape of the invalid filtered rows and a sample of those rows\n# if invalid_rows_df.count() > 0:\n#     print(f\"Number of total rows (after union): {df_matches.count()}\")\n#     print(f\"Number of invalid rows (after union): {invalid_rows_df.count()}\")\n#     # invalid_sample = invalid_rows_df.orderBy(F.col(\"playerId\")).select(\"playerId\", \"matchId\").show(5, truncate=False)\n#     raise ValueError(\"There are more than two rows for some players in some matches (after union)\")\n\n# ###",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 104,
			"outputs": [
				{
					"name": "stdout",
					"text": "Shape after dropping ineligible matches for all players frame: 14727\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### ADD COMPETITION AND SEASON INFO\n# TODO: work out how to pass comp id, season name, season id, then join on comps_df for display name\n# TODO: season start and end are a nice to have too\ncompetition_id = 47 #competition_season_id[0:3]\nseason_id = 281 #competition_season_id[3:]\ncompetition_name = \"JPL\"\nseason_name = \"2024\"\n\n# Append league and season ids as columns\ndf_split = df_split.withColumn(\"competition_id\", lit(competition_id))\\\n            .withColumn(\"season_id\", lit(season_id))\\\n            .withColumn(\"competition_name\", lit(competition_name))\\\n            .withColumn(\"displayName\", lit(competition_name))\\\n            .withColumn(\"season_name\", lit(season_name))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 105,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### COLUMN CREATIONS TO CONFORM TO SILVER CONVENTION incl cAmelCasE\ndf_split = df_split.withColumnRenamed(\"player_id\", \"playerId\")\\\n                        .withColumnRenamed(\"team_id\", \"teamId\")\\\n                        .withColumnRenamed(\"season_id\", \"seasonId\")\\\n                        .withColumnRenamed(\"competition_id\", \"competitionId\")\\\n                        .withColumnRenamed(\"displayName\", \"competitionName\")\\\n                         .withColumnRenamed(\"team_name\", \"teamName\")\\\n                        .withColumnRenamed(\"season_name\", \"seasonDisplayName\")\\\n                         .withColumnRenamed(\"default_name\", \"playerName\")\\\n                         .withColumnRenamed(\"birth_date\", \"birthDate\")\\\n                         .withColumnRenamed(\"player_weight\", \"playerWeight\")\\\n                         .withColumnRenamed(\"player_height\", \"playerHeight\")\n\n                        #.withColumnRenamed(\"season.startDate\", \"seasonStart\")\\ # don't have\n                        #.withColumnRenamed(\"season.endDate\", \"seasonEnd\")\\ # don't have\n                        #.withColumnRenamed(\"nationality\", \"birthArea\") # nationality parsed directly\n            \n### Create additional columns\n# TODO: parse first and last four digits, confirm difference is one\ndf_split = df_split.withColumn(\"startYear\", lit(2023)) #year(to_date(col(\"seasonStart\"), \"yyyy-MM-dd\")))\ndf_split = df_split.withColumn(\"endYear\", lit(2024)) #year(to_date(col(\"seasonEnd\"), \"yyyy-MM-dd\")))\n# TODO: derive seasonName from dates or naming convention udf (last 4 digit string)\n# df_split = df_split.withColumn(\"seasonName\", col(\"endYear\"))\ndf_split = df_split.withColumn(\"seasonName\", col(\"seasonDisplayName\"))\ndf_split = df_split.withColumn(\"competitionShortName\", col(\"seasonDisplayName\")) # TODO: use comps_df Statsbomb name\ndf_split = df_split.withColumn(\"seasonPartition\", col(\"seasonId\"))\ndf_split = df_split.withColumn(\"fullName\", col(\"player_name\")) #F.concat_ws(\" \", df_split[\"firstName\"],  df_split[\"lastName\"]))\ndf_split = df_split.withColumn(\"teamSeason\", F.concat(df_split[\"teamName\"], F.lit(\" \"), df_split[\"seasonName\"]))\ndf_split = df_split.withColumn(\"playerTeamSeason\", F.concat(df_split[\"playerName\"], F.lit(\" \"), df_split[\"teamSeason\"]))\ndf_split = df_split.withColumn(\"playerTeamSeasonCompetition\", F.concat(df_split[\"playerTeamSeason\"], F.lit(\" \"), df_split[\"competitionShortName\"]))\ndf_split = df_split.withColumn(\"profileId\", F.concat(df_split[\"playerId\"].cast(\"string\"), \n                                             df_split[\"teamId\"].cast(\"string\"),\n                                             df_split[\"seasonId\"].cast(\"string\"), \n                                             df_split[\"competitionId\"].cast(\"string\"),\n                                             df_split[\"positionGroup\"]))\n\n# # Apply nationality UDF # nationality parse as statsbomb 'country' designation\n# get_national_team_udf = udf(get_national_team, StringType())\n# df_split = df_split.withColumn(\"nationality\", get_national_team_udf(col(\"currentNationalTeamId\"), col(\"birthArea\")))\n\n# Apply age UDF\ncalculate_age_udf = udf(calculate_age, IntegerType())\ndf_split = df_split.withColumn(\"age\", calculate_age_udf(\"birthDate\", \"endYear\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 106,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# # OPTIONAL: (if running silver per client) read weights from S3\n# try:\n#     weights = pd.read_csv('s3://traits-app/settings/weights/{}.csv'.format(client_id))\n# except FileNotFoundError:\n#     print(\"weights csv file not found\")\n# except pd.errors.EmptyDataError:\n#     print(\"weights csv is empty\")\n\n# required_features = weights['statName'].unique().tolist() \n\n# TODO: refactor below accordingly\n\n# Read in feature formula map\nretrieved_features = pd.read_csv(f's3://traits-app/settings/feature_store_statsbomb.csv')\n\n# Start building the SQL query\nsql_query_features = \"SELECT m.profileId, COUNT(m.profileId) AS sampleSize, SUM(m.player_match_minutes) AS totalMinutesInSample\"\n\n# Iterate through the DataFrame to populate the dictionary\nfor _, row in retrieved_features.iterrows():\n    feature_name = row['feature_name']\n    if pd.notna(row['base_sql']):  # Ensure there is a base SQL to work with\n        print(f\"{feature_name} compiled.\")\n        sql_query_features += f\", {row['base_sql']} AS {feature_name}\"\n    else:\n        print(ValueError(f\"Valid base_sql does not exist for feature '{feature_name}'\"))\n            \n# Finish the SQL query by specifying the source and grouping condition\nsql_query_features += \" FROM {table_name} m GROUP BY m.profileId\"\n\n# Apply the SQL statement\ndf_split.createOrReplaceTempView(\"matches_season\")\ndf_features = spark.sql(sql_query_features.format(table_name='matches_season'))\n\n# Identify aggregationPeriod\ndf_features = df_features.withColumn(\"aggregationPeriod\", lit(\"season\"))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 107,
			"outputs": [
				{
					"name": "stdout",
					"text": "minutes compiled.\nValid base_sql does not exist for feature '90s_played'\naerial_ratio compiled.\naerial_wins_90 compiled.\naggressive_actions_90 compiled.\nValid base_sql does not exist for feature 'appearances'\nassists_90 compiled.\nValid base_sql does not exist for feature 'op_assists_90'\nValid base_sql does not exist for feature 'average_minutes'\nValid base_sql does not exist for feature 'average_x_defensive_action'\naverage_x_pass compiled.\nValid base_sql does not exist for feature 'average_x_pressure'\nbackward_pass_proportion compiled.\nball_recoveries_90 compiled.\nblocks_per_shot compiled.\nbox_cross_ratio compiled.\ncarries_90 compiled.\nValid base_sql does not exist for feature 'carry_length'\ncarry_ratio compiled.\nchallenge_ratio compiled.\nValid base_sql does not exist for feature 'change_in_passing_ratio'\nValid base_sql does not exist for feature 'clcaa'\nclearance_90 compiled.\nconversion_ratio compiled.\nValid base_sql does not exist for feature 'counterpressure_regains_90'\nValid base_sql does not exist for feature 'counterpressures_90'\ncrosses_90 compiled.\ncrossing_ratio compiled.\nValid base_sql does not exist for feature 'da_aggressive_distance'\ndeep_completions_90 compiled.\ndeep_progressions_90 compiled.\nValid base_sql does not exist for feature 'defensive_action_regains_90'\ndispossessions_90 compiled.\ndribble_faced_ratio compiled.\ndribble_ratio compiled.\ndribbled_past_90 compiled.\ndribbles_90 compiled.\nerrors_90 compiled.\nop_f3_backward_pass_proportion compiled.\nop_f3_forward_pass_proportion compiled.\nValid base_sql does not exist for feature 'op_f3_passes_90'\nop_f3_sideways_pass_proportion compiled.\nfailed_dribbles_90 compiled.\nfhalf_ball_recoveries_90 compiled.\nValid base_sql does not exist for feature 'fhalf_counterpressures_90'\nValid base_sql does not exist for feature 'fhalf_counterpressures_ratio'\nValid base_sql does not exist for feature 'fhalf_pressures_90'\nValid base_sql does not exist for feature 'fhalf_pressures_ratio'\nforward_pass_proportion compiled.\nfouls_90 compiled.\nfouls_won_90 compiled.\nValid base_sql does not exist for feature 'goals_90'\nValid base_sql does not exist for feature 'goals_faced_90'\nValid base_sql does not exist for feature 'gsaa_90'\nValid base_sql does not exist for feature 'gsaa_ratio'\ninterceptions_90 compiled.\nkey_passes_90 compiled.\nop_key_passes_90 compiled.\nValid base_sql does not exist for feature 'left_foot_ratio'\nlong_ball_ratio compiled.\nlong_balls_90 compiled.\nValid base_sql does not exist for feature 'minutes'\nValid base_sql does not exist for feature 'most_recent_match'\nValid base_sql does not exist for feature 'np_optimal_gk_dlength'\nValid base_sql does not exist for feature 'np_psxg_faced_90'\nnp_shots_90 compiled.\nnp_xg_90 compiled.\nValid base_sql does not exist for feature 'np_xg_faced_90'\nnp_xg_per_shot compiled.\nnpg_90 compiled.\nnpga_90 compiled.\nValid base_sql does not exist for feature 'npot_psxg_faced_90'\nValid base_sql does not exist for feature 'npxgxa_90'\nnp_psxg_90 compiled.\nValid base_sql does not exist for feature 'obv_90'\nValid base_sql does not exist for feature 'obv_pass_90'\nValid base_sql does not exist for feature 'obv_shot_90'\nValid base_sql does not exist for feature 'obv_defensive_action_90'\nValid base_sql does not exist for feature 'obv_dribble_carry_90'\nValid base_sql does not exist for feature 'obv_gk_90'\nValid base_sql does not exist for feature 'ot_shots_faced_90'\nValid base_sql does not exist for feature 'ot_shots_faced_ratio'\nValid base_sql does not exist for feature 'over_under_performance_90'\nValid base_sql does not exist for feature 'p_pass_length'\npadj_clearances_90 compiled.\npadj_interceptions_90 compiled.\npadj_pressures_90 compiled.\npadj_tackles_90 compiled.\npadj_tackles_and_interceptions_90 compiled.\nValid base_sql does not exist for feature 'pass_into_danger_ratio'\nValid base_sql does not exist for feature 'pass_into_pressure_ratio'\npass_length compiled.\nValid base_sql does not exist for feature 'pass_length_ratio'\nop_passes_90 compiled.\npasses_inside_box_90 compiled.\nValid base_sql does not exist for feature 'op_passes_into_and_touches_inside_box_90'\nop_passes_into_box_90 compiled.\nValid base_sql does not exist for feature 'passes_into_box_90'\npasses_pressed_ratio compiled.\npassing_ratio compiled.\nValid base_sql does not exist for feature 'penalty_conversion_ratio'\nValid base_sql does not exist for feature 'penalties_conceded_90'\nValid base_sql does not exist for feature 'penalties_faced_90'\npenalty_wins_90 compiled.\nValid base_sql does not exist for feature 'positive_outcome_90'\nValid base_sql does not exist for feature 'positive_outcome_score'\npressure_regains_90 compiled.\nValid base_sql does not exist for feature 'pressured_change_in_pass_length'\nValid base_sql does not exist for feature 'pressured_long_balls_90'\nValid base_sql does not exist for feature 'pressured_pass_length_ratio'\nValid base_sql does not exist for feature 'pressured_passing_ratio'\npressures_90 compiled.\nValid base_sql does not exist for feature 'ps_pass_length'\nred_cards_90 compiled.\ns_pass_length compiled.\nValid base_sql does not exist for feature 'save_ratio'\nValid base_sql does not exist for feature 'second_yellow_cards_90'\nshot_on_target_ratio compiled.\nshot_touch_ratio compiled.\nValid base_sql does not exist for feature 'shots_faced_90'\nValid base_sql does not exist for feature 'shots_key_passes_90'\nsideways_pass_proportion compiled.\nValid base_sql does not exist for feature 'sp_assists_90'\nValid base_sql does not exist for feature 'sp_key_passes_90'\nValid base_sql does not exist for feature 'sp_passes_into_box_90'\nValid base_sql does not exist for feature 'sp_xa_90'\nValid base_sql does not exist for feature 'starting_appearances'\ntackles_90 compiled.\ntackles_and_interceptions_90 compiled.\nthrough_balls_90 compiled.\nValid base_sql does not exist for feature 'total_dribbles_90'\ntouches_inside_box_90 compiled.\nturnovers_90 compiled.\nunpressured_long_balls_90 compiled.\nop_xa_90 compiled.\nxa_90 compiled.\nValid base_sql does not exist for feature 'op_xgbuildup'\nValid base_sql does not exist for feature 'op_xgbuildup_90'\nxgbuildup compiled.\nxgbuildup_90 compiled.\nValid base_sql does not exist for feature 'op_xgbuildup_per_possession'\nValid base_sql does not exist for feature 'xgbuildup_per_possession'\nValid base_sql does not exist for feature 'op_xgchain'\nValid base_sql does not exist for feature 'op_xgchain_90'\nxgchain compiled.\nxgchain_90 compiled.\nValid base_sql does not exist for feature 'op_xgchain_per_possession'\nValid base_sql does not exist for feature 'xgchain_per_possession'\nValid base_sql does not exist for feature 'xs_ratio'\nyellow_cards_90 compiled.\nValid base_sql does not exist for feature 'average_space_received_in'\nValid base_sql does not exist for feature 'average_fhalf_space_received_in'\nValid base_sql does not exist for feature 'average_f3_space_received_in'\nValid base_sql does not exist for feature 'ball_receipts_in_space_10_ratio'\nValid base_sql does not exist for feature 'ball_receipts_in_space_2_ratio'\nValid base_sql does not exist for feature 'ball_receipts_in_space_5_ratio'\nValid base_sql does not exist for feature 'fhalf_ball_receipts_in_space_10_ratio'\nValid base_sql does not exist for feature 'fhalf_ball_receipts_in_space_2_ratio'\nValid base_sql does not exist for feature 'fhalf_ball_receipts_in_space_5_ratio'\nValid base_sql does not exist for feature 'f3_ball_receipts_in_space_10_ratio'\nValid base_sql does not exist for feature 'f3_ball_receipts_in_space_2_ratio'\nValid base_sql does not exist for feature 'f3_ball_receipts_in_space_5_ratio'\nValid base_sql does not exist for feature 'lbp_90'\nValid base_sql does not exist for feature 'lbp_completed_90'\nValid base_sql does not exist for feature 'lbp_ratio'\nValid base_sql does not exist for feature 'fhalf_lbp_completed_90'\nValid base_sql does not exist for feature 'fhalf_lbp_ratio'\nValid base_sql does not exist for feature 'f3_lbp_completed_90'\nValid base_sql does not exist for feature 'f3_lbp_ratio'\nValid base_sql does not exist for feature 'fhalf_lbp_90'\nValid base_sql does not exist for feature 'f3_lbp_90'\nValid base_sql does not exist for feature 'obv_lbp_90'\nValid base_sql does not exist for feature 'fhalf_obv_lbp_90'\nValid base_sql does not exist for feature 'f3_obv_lbp_90'\nValid base_sql does not exist for feature 'lbp_pass_ratio'\nValid base_sql does not exist for feature 'fhalf_lbp_pass_ratio'\nValid base_sql does not exist for feature 'f3_lbp_pass_ratio'\nValid base_sql does not exist for feature 'lbp_received_90'\nValid base_sql does not exist for feature 'fhalf_lbp_received_90'\nValid base_sql does not exist for feature 'f3_lbp_received_90'\nValid base_sql does not exist for feature 'average_lbp_to_space_distance'\nValid base_sql does not exist for feature 'fhalf_average_lbp_to_space_distance'\nValid base_sql does not exist for feature 'f3_average_lbp_to_space_distance'\nValid base_sql does not exist for feature 'lbp_to_space_10_received_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_10_received_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_10_received_90'\nValid base_sql does not exist for feature 'lbp_to_space_2_received_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_2_received_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_2_received_90'\nValid base_sql does not exist for feature 'lbp_to_space_5_received_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_5_received_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_5_received_90'\nValid base_sql does not exist for feature 'average_lbp_to_space_received_distance'\nValid base_sql does not exist for feature 'fhalf_average_lbp_to_space_received_distance'\nValid base_sql does not exist for feature 'f3_average_lbp_to_space_received_distance'\nValid base_sql does not exist for feature 'lbp_to_space_10_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_10_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_10_90'\nValid base_sql does not exist for feature 'lbp_to_space_2_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_2_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_2_90'\nValid base_sql does not exist for feature 'lbp_to_space_5_90'\nValid base_sql does not exist for feature 'fhalf_lbp_to_space_5_90'\nValid base_sql does not exist for feature 'f3_lbp_to_space_5_90'\nValid base_sql does not exist for feature '360_minutes'\nValid base_sql does not exist for feature 'defensive_action_90'\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Repeat for last four and eight games\n# TODO: wrap into function which accepts last_x parameter\n# Filter for top 4 and top 8 matches (for each profile, not by gameweek)\nwindowSpec = Window.partitionBy(\"profileId\").orderBy(col(\"match_id\").desc()) # Assumes match ids are ascending\ndf_ranked = df_split.withColumn(\"rank\", rank().over(windowSpec))\ndf_last4 = df_ranked.filter(col(\"rank\") <= 4)\ndf_last8 = df_ranked.filter(col(\"rank\") <= 8)\n\ndf_last4.createOrReplaceTempView(\"matches_4\")\ndf_last8.createOrReplaceTempView(\"matches_8\")\n\ndf_features_4 = spark.sql(sql_query_features.format(table_name=\"matches_4\"))\ndf_features_8 = spark.sql(sql_query_features.format(table_name=\"matches_8\"))\n\ndf_features_4 = df_features_4.withColumn(\"aggregationPeriod\", lit(\"last_four\"))\ndf_features_8 = df_features_8.withColumn(\"aggregationPeriod\", lit(\"last_eight\"))\n\n# NB: pay attention to order of operations here\n# Concatenate all the DataFrames\ndf_features = df_features.unionByName(df_features_4).unionByName(df_features_8)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 108,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Remerge meta info to aggregated DataFrame\n# NB: there may be duplicates on position_name here, hence information loss\ndf_features = df_features.join(df_split.select(meta_cols).dropDuplicates(['profileId']), on=\"profileId\", how=\"left\")\n\n# NOTE: there will be duplicate profileIds at this point\n# Update profileId (represents profile participation and aggregation)\ndf_features = df_features.withColumn(\"profileId\", concat(df_features[\"profileId\"], lit(\"-\"), df_features[\"aggregationPeriod\"]))\n\n# Merge total player-season minutes and appearance counts to profiles\n# NB: this does not apply any thresholding on contributing matches (see upstream)\nany_df = df_features.filter((df_features.positionGroup == 'ANY') & (df_features.aggregationPeriod == 'season')) \nany_df = any_df.withColumnRenamed(\"totalMinutesInSample\", \"totalMinutesForSeason\")\\\n                            .withColumnRenamed(\"sampleSize\", \"appearancesForSeason\")\n\ndf_features = df_features.join(any_df.select(\"playerId\", \"teamId\", \"seasonId\", \"competitionId\", \"totalMinutesForSeason\", \"appearancesForSeason\"),\n                               on=[\"playerId\", \"teamId\", \"seasonId\", \"competitionId\"], how=\"left\") #  TODO: merge on pre-aggregation period profileId\n\nprint_shape(df_features)\n\n# # TODO: Assert no duplicates\n# assert df_features.count() - df_features.dropDuplicates([\"playerTeamSeasonCompetition\", \"aggregationPeriod\"]).count() == 0\n# assert df_features.count() - df_features.dropDuplicates([\"profileId\"]).count() == 0",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 109,
			"outputs": [
				{
					"name": "stdout",
					"text": "AssertionError: \n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print_shape(df_features)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 110,
			"outputs": [
				{
					"name": "stdout",
					"text": "Matches shape: Rows = 8263, Columns = 171\nGames in selected seasons: 272\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print_shape(df_features.dropDuplicates([\"playerTeamSeasonCompetition\", \"aggregationPeriod\"]))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 111,
			"outputs": [
				{
					"name": "stdout",
					"text": "Matches shape: Rows = 8263, Columns = 171\nGames in selected seasons: 272\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print_shape(df_features.dropDuplicates([\"profileId\"]))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 112,
			"outputs": [
				{
					"name": "stdout",
					"text": "Matches shape: Rows = 8263, Columns = 171\nGames in selected seasons: 272\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### Enforce output table data types\n# NB: all unspecified columns will be floats for simplicity\n\nfor column_name in df_features.columns:\n    if column_name in integer_columns:\n        df_features = df_features.withColumn(column_name, col(column_name).cast('integer'))\n    elif column_name in string_columns:\n        df_features = df_features.withColumn(column_name, col(column_name).cast('string'))\n    else:\n        df_features = df_features.withColumn(column_name, col(column_name).cast('double'))",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 113,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Basic write DataFrame to S3 with overwrite mode\ndf_features.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"seasonPartition\") \\\n    .format(\"parquet\") \\\n    .option(\"compression\", \"snappy\") \\\n    .save(f\"s3://traits-app/silver-cleaned/{client_id}\")\n\n# TODO: add logs incl. schema pre-write, post-enforcement\ndf_features.printSchema()\n\nprint(\"Write successful.\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 114,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- playerId: string (nullable = true)\n |-- teamId: string (nullable = true)\n |-- seasonId: string (nullable = true)\n |-- competitionId: string (nullable = true)\n |-- profileId: string (nullable = true)\n |-- sampleSize: integer (nullable = false)\n |-- totalMinutesInSample: integer (nullable = true)\n |-- minutes: double (nullable = true)\n |-- aerial_ratio: double (nullable = true)\n |-- aerial_wins_90: double (nullable = true)\n |-- aggressive_actions_90: double (nullable = true)\n |-- assists_90: double (nullable = true)\n |-- average_x_pass: double (nullable = true)\n |-- backward_pass_proportion: double (nullable = true)\n |-- ball_recoveries_90: double (nullable = true)\n |-- blocks_per_shot: double (nullable = true)\n |-- box_cross_ratio: double (nullable = true)\n |-- carries_90: double (nullable = true)\n |-- carry_ratio: double (nullable = false)\n |-- challenge_ratio: double (nullable = true)\n |-- clearance_90: double (nullable = true)\n |-- conversion_ratio: double (nullable = true)\n |-- crosses_90: double (nullable = true)\n |-- crossing_ratio: double (nullable = true)\n |-- deep_completions_90: double (nullable = true)\n |-- deep_progressions_90: double (nullable = true)\n |-- dispossessions_90: double (nullable = true)\n |-- dribble_faced_ratio: double (nullable = true)\n |-- dribble_ratio: double (nullable = true)\n |-- dribbled_past_90: double (nullable = true)\n |-- dribbles_90: double (nullable = true)\n |-- errors_90: double (nullable = true)\n |-- op_f3_backward_pass_proportion: double (nullable = true)\n |-- op_f3_forward_pass_proportion: double (nullable = true)\n |-- op_f3_sideways_pass_proportion: double (nullable = true)\n |-- failed_dribbles_90: double (nullable = true)\n |-- fhalf_ball_recoveries_90: double (nullable = true)\n |-- forward_pass_proportion: double (nullable = true)\n |-- fouls_90: double (nullable = true)\n |-- fouls_won_90: double (nullable = true)\n |-- interceptions_90: double (nullable = true)\n |-- key_passes_90: double (nullable = true)\n |-- op_key_passes_90: double (nullable = true)\n |-- long_ball_ratio: double (nullable = true)\n |-- long_balls_90: double (nullable = true)\n |-- np_shots_90: double (nullable = true)\n |-- np_xg_90: double (nullable = true)\n |-- np_xg_per_shot: double (nullable = true)\n |-- npg_90: double (nullable = true)\n |-- npga_90: double (nullable = true)\n |-- np_psxg_90: double (nullable = true)\n |-- padj_clearances_90: double (nullable = true)\n |-- padj_interceptions_90: double (nullable = true)\n |-- padj_pressures_90: double (nullable = true)\n |-- padj_tackles_90: double (nullable = true)\n |-- padj_tackles_and_interceptions_90: double (nullable = true)\n |-- pass_length: double (nullable = true)\n |-- op_passes_90: double (nullable = true)\n |-- passes_inside_box_90: double (nullable = true)\n |-- op_passes_into_box_90: double (nullable = true)\n |-- passes_pressed_ratio: double (nullable = true)\n |-- passing_ratio: double (nullable = true)\n |-- penalty_wins_90: double (nullable = true)\n |-- pressure_regains_90: double (nullable = true)\n |-- pressures_90: double (nullable = true)\n |-- red_cards_90: double (nullable = true)\n |-- s_pass_length: double (nullable = true)\n |-- shot_on_target_ratio: double (nullable = true)\n |-- shot_touch_ratio: double (nullable = true)\n |-- sideways_pass_proportion: double (nullable = true)\n |-- tackles_90: double (nullable = true)\n |-- tackles_and_interceptions_90: double (nullable = true)\n |-- through_balls_90: double (nullable = true)\n |-- touches_inside_box_90: double (nullable = true)\n |-- turnovers_90: double (nullable = true)\n |-- unpressured_long_balls_90: double (nullable = true)\n |-- op_xa_90: double (nullable = true)\n |-- xa_90: double (nullable = true)\n |-- xgbuildup: double (nullable = true)\n |-- xgbuildup_90: double (nullable = true)\n |-- xgchain: double (nullable = true)\n |-- xgchain_90: double (nullable = true)\n |-- yellow_cards_90: double (nullable = true)\n |-- aggregationPeriod: string (nullable = false)\n |-- seasonPartition: string (nullable = true)\n |-- startYear: integer (nullable = true)\n |-- endYear: integer (nullable = true)\n |-- age: integer (nullable = true)\n |-- playerName: string (nullable = true)\n |-- teamName: string (nullable = true)\n |-- seasonName: string (nullable = true)\n |-- seasonDisplayName: string (nullable = true)\n |-- competitionName: string (nullable = true)\n |-- competitionShortName: string (nullable = true)\n |-- fullName: string (nullable = true)\n |-- nationality: string (nullable = true)\n |-- birthDate: string (nullable = true)\n |-- positionGroup: string (nullable = true)\n |-- positionName: string (nullable = true)\n |-- positionAbbreviation: string (nullable = true)\n |-- teamSeason: string (nullable = true)\n |-- playerTeamSeason: string (nullable = true)\n |-- playerTeamSeasonCompetition: string (nullable = true)\n |-- totalMinutesForSeason: integer (nullable = true)\n |-- appearancesForSeason: integer (nullable = true)\n\nWrite successful.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "### COMMENT OUT\ndf_features.toPandas().to_csv(f\"s3://traits-app/silver-cleaned/{client_id}/statsbomb_features_test.csv\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 115,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		}
	]
}